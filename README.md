# Text_Summarization

To build an application that produces notes based on transcripts generated during online classes across different meeting platforms, we had shortlisted 6 Summarization # algorithms to start with. From simple pretrained architectural summarizers to advanced Transformer-based models that's been trained on huge corpuses. After experimenting with the algorithms, we analysed the results were very good in transformer-based summarizers like, BERT (Bidirectional Encoder Representations from Transformers) and GPT-2. The dataset we chose were raw transcripts from online lectures. It had speaker name, time stamp and the textual content. A preprocessing function was written to read the raw .txt file and was converted to a usable, cleaned string. The cleaning function contains lowering the text, removal of numbers and extra spaces. Stop words are then removed from the sentences. Finally, each word is lemmatized to its origin form and sent as a preprocessed text
